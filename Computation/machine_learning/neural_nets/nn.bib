@book{Clarke2009,
abstract = {A more theoretical book on the same subject as the book on statistical learning by Hastie/Tibshirani/Friedman},
author = {Clarke, Bertrand and Fokoue, Ernest and Zhang, Hao Helen},
booktitle = {Learning},
doi = {10.1007/978-0-387-98135-2},
isbn = {9780387981345},
issn = {01727397},
number = {2003},
pages = {251--264},
pmid = {15772297},
title = {{Principles and Theory for Data Mining and Machine Learning}},
url = {http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-98134-5?cm\_mmc=AD-\_-Enews-\_-ECS12245\_V1-\_-978-0-387-98134-5},
volume = {26},
year = {2009}
}
@article{Barron1994,
author = {Barron, Andrew R.},
doi = {10.1007/BF00993164},
file = {:C$\backslash$:/Users/Owner/Dropbox/Math/Computation/machine\_learning/neural\_nets/approximation and estimation bounds for artificial neural networks.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Neural nets,approximation theory,complexity regularization,estimation theory,statistical risk},
number = {1},
pages = {115--133},
title = {{Approximation and estimation bounds for artificial neural networks}},
volume = {14},
year = {1994}
}
@article{Barron1993,
abstract = {Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order <e1>O </e1>(1/<e1>n</e1>), where <e1>n</e1> is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with <e1>n</e1> terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/<e1>n</e1><sup>2</sup>d/ uniformly for functions satisfying the same smoothness assumption, where <e1>d</e1> is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings},
author = {Barron, Andrew R.},
doi = {10.1109/18.256500},
file = {:C$\backslash$:/Users/Owner/Dropbox/Math/Computation/machine\_learning/neural\_nets/93.Barron.Universal.pdf:pdf},
isbn = {0-7803-0056-4},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
number = {3},
pages = {930--945},
title = {{Universal approximation bounds for superpositions of a sigmoidal function}},
volume = {39},
year = {1993}
}
@article{Telgarsky1993,
author = {Telgarsky, Matus},
file = {:C$\backslash$:/Users/Owner/Dropbox/Math/Computation/machine\_learning/neural\_nets/matus.pdf:pdf},
title = {{Representation Power of Feedforward Neural Networks Feedforward Neural Networks}},
year = {1993}
}
